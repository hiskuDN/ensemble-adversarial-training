{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train mnist models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "from tensorflow.python.ops import init_ops\n",
    "\n",
    "from tensorflow.python.platform import flags\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "import tensorflow._api.v2.compat.v1 as tf\n",
    "\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define tf util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_FREQUENCY = 100\n",
    "\n",
    "def error_rate(predictions, labels):\n",
    "    \"\"\"\n",
    "    Return the error rate in percent.\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(predictions) == len(labels)\n",
    "\n",
    "    return 100.0 - (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])\n",
    "\n",
    "def batch_eval(tf_inputs, tf_outputs, numpy_inputs):\n",
    "    \"\"\"\n",
    "    A helper function that computes a tensor on numpy inputs by batches.\n",
    "    From: https://github.com/openai/cleverhans/blob/master/cleverhans/utils_tf.py\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(numpy_inputs)\n",
    "    assert n > 0\n",
    "    assert n == len(tf_inputs)\n",
    "    m = numpy_inputs[0].shape[0]\n",
    "    for i in range(1, n):\n",
    "        assert numpy_inputs[i].shape[0] == m\n",
    "\n",
    "    out = []\n",
    "    for _ in tf_outputs:\n",
    "        out.append([])\n",
    "\n",
    "    for start in range(0, m, 10):\n",
    "        batch = start // 10\n",
    "\n",
    "        # Compute batch start and end indices\n",
    "        start = batch * 10\n",
    "        end = start + 10\n",
    "        numpy_input_batches = [numpy_input[start:end]\n",
    "                               for numpy_input in numpy_inputs]\n",
    "        cur_batch_size = numpy_input_batches[0].shape[0]\n",
    "        assert cur_batch_size <= 10\n",
    "        for e in numpy_input_batches:\n",
    "            assert e.shape[0] == cur_batch_size\n",
    "\n",
    "        feed_dict = dict(zip(tf_inputs, numpy_input_batches))\n",
    "        feed_dict[K.learning_phase()] = 0\n",
    "        numpy_output_batches = K.get_session().run(tf_outputs,\n",
    "                                                   feed_dict=feed_dict)\n",
    "        for e in numpy_output_batches:\n",
    "            assert e.shape[0] == cur_batch_size, e.shape\n",
    "        for out_elem, numpy_output_batch in zip(out, numpy_output_batches):\n",
    "            out_elem.append(numpy_output_batch)\n",
    "\n",
    "    out = [np.concatenate(x, axis=0) for x in out]\n",
    "    for e in out:\n",
    "        assert e.shape[0] == m, e.shape\n",
    "    return out\n",
    "\n",
    "def gen_adv_loss(logits, y, loss='logloss', mean=False):\n",
    "    \"\"\"\n",
    "    Generate the loss function.\n",
    "    \"\"\"\n",
    "\n",
    "    if loss == 'training':\n",
    "        # use the model's output instead of the true labels to avoid\n",
    "        # label leaking at training time\n",
    "        y = K.cast(K.equal(logits, K. max(logits, 1, keepdims=True)), \"float32\")\n",
    "        y = y / K.sum(y, 1, keepdims=True)\n",
    "        out = K.categorical_crossentropy(logits, y, from_logits=True)\n",
    "    elif loss == 'logloss':\n",
    "        out = K.categorical_crossentropy(logits, y, from_logits=True)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown loss: {}\".format(loss))\n",
    "\n",
    "    if mean:\n",
    "        out = K.mean(out)\n",
    "    else:\n",
    "        out = K.sum(out)\n",
    "    return out\n",
    "\n",
    "def tf_train(x, y, model, X_train, Y_train, generator, x_advs=None, num_of_epochs=0):\n",
    "    old_vars = set(tf.global_variables())\n",
    "    train_size = Y_train.shape[0]\n",
    "\n",
    "    # Generate cross-entropy loss for training\n",
    "    logits = model(x)\n",
    "    preds = K.softmax(logits)\n",
    "    l1 = gen_adv_loss(logits, y, mean=True)\n",
    "\n",
    "    # add adversarial training loss\n",
    "    if x_advs is not None:\n",
    "        idx = tf.placeholder(dtype=np.int32)\n",
    "        logits_adv = model(tf.stack(x_advs)[idx])\n",
    "        l2 = gen_adv_loss(logits_adv, y, mean=True)\n",
    "        loss = 0.5*(l1+l2)\n",
    "    else:\n",
    "        l2 = tf.constant(0)\n",
    "        loss = l1\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "    # Run all the initializers to prepare the trainable parameters.\n",
    "    K.get_session().run(tf.initialize_variables(\n",
    "        set(tf.all_variables()) - old_vars))\n",
    "    start_time = time.time()\n",
    "    print('Initialized!')\n",
    "\n",
    "    # Loop through training steps.\n",
    "    num_steps = int(num_of_epochs * train_size + 10 - 1) // 10\n",
    "\n",
    "    step = 0\n",
    "    for (batch_data, batch_labels) \\\n",
    "            in generator.flow(X_train, Y_train, batch_size=10):\n",
    "\n",
    "        if len(batch_data) < 10:\n",
    "            k = 10 - len(batch_data)\n",
    "            batch_data = np.concatenate([batch_data, X_train[0:k]])\n",
    "            batch_labels = np.concatenate([batch_labels, Y_train[0:k]])\n",
    "        \n",
    "        feed_dict = {x: batch_data,\n",
    "                     y: batch_labels,\n",
    "                     K.learning_phase(): 1}\n",
    "\n",
    "        # choose source of adversarial examples at random\n",
    "        # (for ensemble adversarial training)\n",
    "        if x_advs is not None:\n",
    "            feed_dict[idx] = np.random.randint(len(x_advs))\n",
    "\n",
    "        # Run the graph\n",
    "        _, curr_loss, curr_l1, curr_l2, curr_preds, _ = \\\n",
    "            K.get_session().run([optimizer, loss, l1, l2, preds]\n",
    "                                + [model.updates],\n",
    "                                feed_dict=feed_dict)\n",
    "\n",
    "        if step % EVAL_FREQUENCY == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            start_time = time.time()\n",
    "            print('Step %d (epoch %.2f), %.2f s' %\n",
    "                (step, float(step) * 10 / train_size,\n",
    "                 elapsed_time))\n",
    "            print('Minibatch loss: %.3f (%.3f, %.3f)' % (curr_loss, curr_l1, curr_l2))\n",
    "\n",
    "            print('Minibatch error: %.1f%%' % error_rate(curr_preds, batch_labels))\n",
    "\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        step += 1\n",
    "        if step == num_steps:\n",
    "            break\n",
    "def tf_test_error_rate(model, x, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Compute test error.\n",
    "    \"\"\"\n",
    "    assert len(X_test) == len(y_test)\n",
    "\n",
    "    # Predictions for the test set\n",
    "    eval_prediction = K.softmax(model(x))\n",
    "\n",
    "    predictions = batch_eval([x], [eval_prediction], [X_test])[0]\n",
    "\n",
    "    return error_rate(predictions, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get mnist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_mnist(one_hot=True):\n",
    "    \"\"\"\n",
    "    Preprocess MNIST dataset\n",
    "    \"\"\"\n",
    "    # the data, shuffled and split between train and test sets\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "    X_train = X_train.reshape(X_train.shape[0],\n",
    "                              28,\n",
    "                              28,\n",
    "                              1)\n",
    "\n",
    "    X_test = X_test.reshape(X_test.shape[0],\n",
    "                            28,\n",
    "                            28,\n",
    "                            1)\n",
    "\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "    X_train /= 255\n",
    "    X_test /= 255\n",
    "    print('X_train shape:', X_train.shape)\n",
    "    print(X_train.shape[0], 'train samples')\n",
    "    print(X_test.shape[0], 'test samples')\n",
    "\n",
    "    print(\"Loaded MNIST test data.\")\n",
    "\n",
    "    if one_hot:\n",
    "        # convert class vectors to binary class matrices\n",
    "        y_train = np_utils.to_categorical(y_train, 10).astype(np.float32)\n",
    "        y_test = np_utils.to_categorical(y_test, 10).astype(np.float32)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def data_gen_mnist(X_train):\n",
    "    datagen = ImageDataGenerator()\n",
    "\n",
    "    datagen.fit(X_train)\n",
    "    return datagen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get mnist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Loaded MNIST test data.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# configure tensorflow v1 and keras backend\n",
    "tf.disable_v2_behavior()\n",
    "    \n",
    "np.random.seed(0)\n",
    "assert keras.backend.backend() == \"tensorflow\"\n",
    "\n",
    "\n",
    "# get mnist data here\n",
    "X_train, Y_train, X_test, Y_test = data_mnist()\n",
    "\n",
    "data_gen = data_gen_mnist(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = K.placeholder((None,\n",
    "                       28,\n",
    "                       28,\n",
    "                       1\n",
    "                       ))\n",
    "\n",
    "y = K.placeholder(shape=(None, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define all models from A to D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelA():\n",
    "    # model = Sequential()\n",
    "    # model.add(Convolution2D(64, kernel_size=(5, 5),\n",
    "    #                         padding='valid',\n",
    "    #                         input_shape=(28,\n",
    "    #                                      28,\n",
    "    #                                      1)))\n",
    "    # model.add(Activation('relu'))\n",
    "\n",
    "    # model.add(Convolution2D(64, kernel_size=(5, 5)))\n",
    "    # model.add(Activation('relu'))\n",
    "\n",
    "    # model.add(Dropout(0.25))\n",
    "\n",
    "    # model.add(Flatten())\n",
    "    # model.add(Dense(128))\n",
    "    # model.add(Activation('relu'))\n",
    "\n",
    "    # model.add(Dropout(0.5))\n",
    "    # model.add(Dense(10))\n",
    "\n",
    "    model=Sequential()\n",
    "\n",
    "    #model.add(Lambda(standardize,input_shape=(28,28,1)))    \n",
    "    model.add(Convolution2D(filters=64, kernel_size = (5, 5), activation=\"relu\", input_shape=(28,28,1)))\n",
    "    model.add(Convolution2D(filters=64, kernel_size = (5, 5), activation=\"relu\"))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Convolution2D(filters=128, kernel_size = (5, 5), activation=\"relu\"))\n",
    "    model.add(Convolution2D(filters=128, kernel_size = (5, 5), activation=\"relu\"))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())    \n",
    "    model.add(Convolution2D(filters=256, kernel_size = (5, 5), activation=\"relu\"))\n",
    "        \n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        \n",
    "    model.add(Flatten())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(512,activation=\"relu\"))\n",
    "        \n",
    "    model.add(Dense(10,activation=\"softmax\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "def model_mnist(type=1):\n",
    "    \"\"\"\n",
    "    Defines MNIST model using Keras sequential model\n",
    "    \"\"\"\n",
    "\n",
    "    models = [modelA]\n",
    "\n",
    "    return models[type]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define model mnist using keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_mnist(type='models/modelA')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60e7106900bc7002996951eff51b9cbbafae0119d98382f0e51dbd2de912a9a5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
